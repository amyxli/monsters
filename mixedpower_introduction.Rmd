---
title: "Introduction to mixedpower"
output: html_document
---
Kumle, L., Vo, M. L-H.., & Draschkow, D.

latest update: March 2019

***
***

This Notebook functions as an introduction for the R package [*mixedpower*](https://github.com/DejanDraschkow/mixedpower) (Kumle, Vo & Draschkow, 2018) designed to perform simulation-based power analysis in (generalized) liner mixed-effect models (LMMs and GLMMs).

Sufficient power and precision in confirmatory analyses is important for the reliability, replicability, and interpretation of empirical findings. Calculating power, however, is not necessarily a trivial task and in certain cases this might pose a feasibility barrier to scientists. One of these cases is power analysis for (generalized) liner mixed-effect models (LMMs and GLMMs). They are a powerful tool for modelling fixed and random effects simultaneously, but do not offer a feasible analytic solution to estimate the probability that a test correctly rejects the null hypothesis. This requires a simulation-based solution. 

In this tutorial, we provide a step-by-step workflow for simulation-based power analyses with mixedpower and introduce different functions included in this package. For more background information on how to appropriatly use this tool in different scenarios see Kumle, Vo, & Draschkow (in preparation).


***
***

###**Simulation-based power analysis**

Simulation-based power analyses are a flexible and intuitive alternative approach to analytic power solutions (Thomas & Juanes, 1996). In simple terms, one basic quastion behind power analysis is:

> Suppose there really is an effect and I run my experiment one hundred times - how many times will I get a statistically significant result? (Coppock, 2013)

The shared underlying principle of the simulation-based power analyses solutions in mixedpower can be broken down in the following steps: 

1. simulation of new data sets
2. analysis of each data set and test for statistical significance and 
3. calculation of the proportion of significant to all simulations

If the simulation of an experiment is accurate, then the probability of getting a significant result in this simulation is similar to the probability of the underlying real experiment (Thomas & Juanes, 1996). 

***
***

###**Power Analysis with mixedpower**

Before we can start the power analysis, mixedpower needs to be installed. The package is hosted on [GitHub](https://github.com/DejanDraschkow/mixedpower) and the devtools package is required to install it. 

```{r eval = F}
# install mixedpower package
if (!require("devtools")) {
    install.packages("devtools", dependencies = TRUE)}
devtools::install_github("DejanDraschkow/mixedpower") 

```

&nbsp;

#### **A function for calculating power** 

Once the library is loaded, we can use the main function mixedpower() to perform a power analysis. 
```{r eval=FALSE, message=FALSE, warning=FALSE}
library(mixedpower)

#core function to perform power analysis
mixedpower(model_emp, data_emp, subvar,fixed_effects, 
           sample_sizes, n_sim, critical_value, confidence_level,
           databased = T, safeguard = T, rnorm = F)

```

In the following sections, we will elaborate on each argument of the function to conceptualize the inner workings of the analysis. Some of these arguments could be extracted automatically from the fitted model, however, we choose to provide a certain explicitness for clarity. Which argument values were chosen would need to be reported by the researcher to provide a transparent description of the procedure. The help function (> ?mixedpower) can be explored for a more concise treatment. 

&nbsp;


#### **Empirical data and model**

Informing the parameters of the simulation is an extremely crucial step as they determine how accurate the simulation represents the underlying experiment. Mixedpower is aimed at GLMMs/LMMs fitted with lme4 (Bates, Maechler, Bolker & Walker, 2015) ??? a power analysis in this context uses such a model to inform the simulation. Thus, a power analysis in mixedpower starts with such a model. 

As an example, we will use a simple model fitted to some hypothetical data called "exemplarData". 
**Note**: Mixedpower can handle much more complicated models with a more complex fixed and random effect structure.

```{r eval = F}
# exemplar model
exemplarModel <- lmer(x ~ fixef1 * fixef2 + 
                    (1|Subject) + (1|Item), 
                    data = exemplarData)
```

Accordently, the arguments *model_emp* and *data_emp* refer to the model used to inform the simulation and the data the model is fitted on. 

```{r eval = F}
# model and data used to inform simulation 
model_emp <- exemplarModel
data_emp <- exemplarData
```

 
The mixedpower()-function requires further specifics about the data and model at hand in order to include all necessary information into the simulation. As mixedpower will simulate new data sets, it is important to ensure that all specifications like customized contrasts on fixed effects and covariates are transferred to the simulated data. Therefor, we need to provide the names of the fixed effects and covariates we have included in our model so that the mixedpower() function will check and transfer all specified contrasts. For this, the argument *fixed_effects* is used. 

Moreover, the argument *subvar* expects the name of the column containing the participant identifier (subvar = "Subject"). Besides determining how many subjects the provided data set contains, this is used to correctly integrate the simulated data into the structure of the provided empirical data set. 

```{r eval = F}
# include relevant information 
subvar <- "Subject"
fixed_effects <- c("fixef1", "fixef2")
```

&nbsp;

#### **Simulation of multiple data sets**

Based on this, mixedpower generates completely new response values. In each iteration of the simulation a new dataset is simulated using the lme4::simulate.merMod() function and the information provided about the data and model at hand.

As noted before, power in a simulation-based approach is computed as the proportion of significant simulations to all simulations. Thus, we need to define the number of simulations we want to run using the *n_sim* argument. More simulations hereby mean less random fluctuation in the results but also longer runtimes for the analysis to complete. As a compromise, we suggest running at least 1000 simulations but fewer runs can be used to check if the analysis is running smoothly. 


Besides generating new simulated response variables, mixedpower allows to specify the number of participants the simulated data set should contain and therefor allows to inspect power as a function of sample size. Ultimately, the comparison of power for different sample sizes can be used to decide on a sample size necessary for adequate power. To define the possible sample sizes the sample_sizes argument can be used. Using the original sample size of the study on hand as an anchor, a range of plausible sample sizes can be specified and n_sim simulations will be conducted for each sample size. 



```{r}
# simulation parameters
n_sim <- 1000
sample_sizes <- c(10, 20, 30, 40)
```

&nbsp;

#### **Calculation of power**

The evaluation of significant simulations to all simulations is done individually for each effect in the model output and as a result of this, mixedpower delivers a power value for every effect specified in the mixed model of interest. 
Behind the scenes of the mixedpower()- function, in each iteration the specified empirical model (model_emp) is refitted with a simulated data set while all other parameters stay the same. The refitted model is evaluated in terms of statistical significance and the outcome is stored for each effect in a binary way (i.e. significant or non-significant). Once all iterations are done, the proportion of significant to all simulations is computed. Mixedpower then continues to simulate power for the next specified sample size repeating this process but simulating data sets with another sample size. 


&nbsp;

#### **Test of statistical significance** 

Deciding if an effect exceeds a significance threshold therefor is a critical aspect of every simulation-based power analysis and heavily influences its outcome. In general, rising this threshold will lead to lower estimated power as it becomes harder to reach it and vice versa.  Mixedpower relies on lme4, which does not provide p-values. Even though there are methods available to compute p-values in mixed models, they are affiliated with ambiguity because degrees of freedom are hard to determine in mixed models and mixedpower therefore works with the available t values for linear mixed models or z values for generalized linear mixed models. Concluding, we need to set a critical_value in form of a t or z  values depending on the model on hand and all coefficients exceeding this value would be counted as significant. In the present example, we decided to set critical_value = 2.

```{r}
# set critical_value
critical_value <- 2
```


&nbsp;

#### **Protection against bias in data**

Again, the accuracy of a simulation-based calculation of power and its interpretability is dependent on how well the model is capturing the data (Thomas & Juanes, 1996) and on how well the data is capturing the underlying effect of interest. Especially data consisting of only few observations or participants, (e.g. from pilot or exploratory studies) carry the risk of uncertainty in respect to effects in the data. Moreover, reproducibility in psychology is known to be threatened by global phenomena, such as questionable research practices or publication bias (i.e. significant results are more likely to be published) resulting in unreliable effect size estimates. 

As the simulations are built upon a model fit on the provided data, it is crucial to be able to correct for a possible uncertainty or bias of the sample at hand (Albers & Lakens, 2018; Perugini, Gallucci, & Costantini, 2014). We offer two solutions for more conservative power analyses (Albers & Lakens, 2018) correcting a possible uncertainty or bias in the data (safeguard = T, rnorm = F) of which one is experimental (rnorm) and will not be discussed here. Perugini et al. (2014) introduced safeguard power, in which a confidence interval is computed around the effect size estimator and the lower bound of this confidence interval is selected to inform a more conservative power analysis (Perugini et al., 2014). Not only is this approach a general conservative solution, it also accounts for the sample size of the data as confidence intervals become narrower as sample size increases (Perugini et al., 2014). The safeguard option in mixedpower borrows this approach and offers the possibility to integrate it in the power analysis process using the confidence_level argument. Again, mixedpower forgoes to compute traditional effect sizes as they are very difficult to compute in mixed models. To adapt the safeguard option, confidence intervals are computed directly around the beta coefficients using the stats::confint()-function. Then, the model output is modified so that the lower boundaries of the computed confidence intervals are used as new coefficients for the effects. Setting the width of the confidence interval used for the safeguard option hereby provides the opportunity to decide on how conservative a researcher wants to be. Larger confidence intervals will lead to more conservative simulations which in turn will result in lower power values and larger proposed sample sizes for adequate power and vice versa. 

In the present example we will use confidence_level = 0.68 to reflect a confidence interval with a width of one standard deviation.

```{r}
# set width of confidence interval
confidence_level <- 0.68
```

It is important to note that safeguard power in mixedpower is available for significant effects only. As confidence intervals are computed directly around beta coefficients in the model and non-significant effects are expected to be closer to zero, computing confidence intervals could lead to boundaries that are further away from zero than the original beta coefficient resulting in a more liberal simulation. Non-significant effects therefore provide a more conservative estimate when not using safeguard power. 

&nbsp;

#### **Power simulation**

We now can run the power simulation for the data at hand. For this, we will use the function mixedpower() which handles and combines simulations for all specified sample sizes and provides us with a power estimate derived from the actual effect sizes of the data (databased), as well as the conservative estimate of safeguard power (safeguard). The relevant parameters specified in the steps above will be used.


```{r eval=FALSE, message=FALSE, warning=FALSE}
power_output <- mixedpower(model_emp = exemplarModel, data_emp = exemplarModel, subvar = "Subject", 
                           fixed_effects = c("fixef1", "fixef2"), critical_value = 2,
                           sample_sizes = c(10,20,30,40), n_sim = 1000, confidence_level = 0.68,
                           databased = T, safeguard = T, rnorm = F)
```


&nbsp;


####**Interpreting and visualizing the power analysis **

The output provided by mixedpower()  contains power values for every effect and all specified sample sizes. The power values are provided both from the original effect sizes (databased) or the corrected conservative estimates (safeguard). 

```{r eval = F}
power_output
```
What do we see here?

power_output contains power values for every effect and all sample sizes we specified. Additionaly, power_output includes the mode (databased and safeguard) to specify under which condition the simulation was run. 

Plotting the output allows us to better understand it. For this, mixedpower provides a function called multiplotPower which plots power as a function of sample size for all effects included in power_output.

**Note**: multiplotPower directly saves the plot in the current working directory. Check *getwd()* to find it.

```{r eval=FALSE, warning=FALSE}
# plot power_output
multiplotPower(power_output)
```


![](/Users/leah/Dropbox/Power/notebooks/mixedpower intro/mixedpower_intro_multiplot.png)

A separate graph for each model coefficient is plotted allowing to inspect power as a function of sample size. Ultimatly, this can be used to decide on a sample size that accounts for adequate power in the planned study. 

***
***

### **References**

Albers, C., & Lakens, D. (2018). When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias. Journal of Experimental Social Psychology. https://doi.org/10.1016/j.jesp.2017.09.004

Coppock, A. (2013). 10 Things to Know About Statistical Power. Retrieved September 20, 2018, from http://egap.org/methods-guides/10-things-you-need-know-about statistical-power

Douglas Bates, Martin Maechler, Ben Bolker, Steve Walker (2015). Fitting Linear
Mixed-Effects Models Using lme4. Journal of Statistical Software, 67(1), 1-48.
doi:10.18637/jss.v067.i01.

Kumle, L., Vo, M. L-H.., & Draschkow, D. (2018). Mixedpower: a library for 
estimating simulation-based power for mixed models in R. https://doi.org/10.5281/zenodo.1341047

Kumle, L., Vo, M. L-H., & Draschkow, D. (in preparation). Estimating power in linear and generalized linear mixed models: an open introduction and tutorial in R.

Perugini, M., Gallucci, M., & Costantini, G. (2014). Safeguard Power as a Protection Against Imprecise 
Power Estimates. Perspectives on Psychological Science, 9(3), 319-332. https://doi.org/10.1177/1745691614528519

Thomas, L., & Juanes, F. (1996). The importance of statistical power analysis: An example from Animal Behaviour. Animal Behaviour, 52(4), 856-859. https://doi.org/10.1006/anbe.1996.0232

